{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "projecttitle = 'Analogy'\n",
    "import sys, os\n",
    "# change this to your path\n",
    "sys.path.append(os.path.join(\"/Users\", \"adam1brownell\", \"Documents\", \"GitHub\", \"task-fmri-utils\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\adam1brownell'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\adam1brownell' + '\\Documents\\Dev101\\MontiLab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fmri_core import analysis as pa\n",
    "from fmri_core import utils as pu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "\n",
    "subj = \"01\" #Within Subject Models\n",
    "\n",
    "#AB Data Load\n",
    "abLoad = scipy.io.loadmat(\"sub-\"+subj+\"_task-analogy_ab-betas-ba10.mat\")\n",
    "abData    = abLoad[\"data\"]      # actual data (numpy array)\n",
    "abAnalogy = abLoad[\"trial_id\"]  # actual analogy they saw\n",
    "abArray   = abLoad[\"main_rel\"]  # main relationship (1,2,3)\n",
    "abRel     = [abArray[0,i][0]for i in range(288)] #I'm cheating and now there are 288 examples\n",
    "abSub     = abLoad[\"sub_rel\"]   # sub relationship (1-9)\n",
    "\n",
    "#CD Data Load\n",
    "cdLoad = scipy.io.loadmat(\"sub-\"+subj+\"_task-analogy_cd-betas-ba10.mat\")\n",
    "cdData    = cdLoad[\"data\"]      # actual data (numpy array)\n",
    "cdAnalogy = cdLoad[\"trial_id\"]  # actual analogy they saw\n",
    "cdArray   = cdLoad[\"main_rel\"]  # main relationship (1,2,3)\n",
    "cdRel     = [cdArray[0,i][0]for i in range(288)] #I'm cheating and now there are 288 examples\n",
    "cdSub     = cdLoad[\"sub_rel\"]   # sub relationship (1-9)\n",
    "\n",
    "\n",
    "\n",
    "#for i in data:\n",
    "#\tif '__' not in i and 'readme' not in i:\n",
    "#\t\tnp.savetxt(\"sub14_ab\"+i+\".csv\",data[i],delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale + Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Small Alpha</th>\n",
       "      <th>Medium Alpha</th>\n",
       "      <th>Large Alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.004345</td>\n",
       "      <td>-0.004345</td>\n",
       "      <td>-0.004351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.004591</td>\n",
       "      <td>-0.004589</td>\n",
       "      <td>-0.004572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.012339</td>\n",
       "      <td>0.012302</td>\n",
       "      <td>0.011950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005145</td>\n",
       "      <td>0.005133</td>\n",
       "      <td>0.005017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.004154</td>\n",
       "      <td>0.004154</td>\n",
       "      <td>0.004155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.008317</td>\n",
       "      <td>0.008304</td>\n",
       "      <td>0.008185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001916</td>\n",
       "      <td>0.001919</td>\n",
       "      <td>0.001946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.001244</td>\n",
       "      <td>0.001256</td>\n",
       "      <td>0.001367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.012271</td>\n",
       "      <td>-0.012249</td>\n",
       "      <td>-0.012034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.009887</td>\n",
       "      <td>-0.009864</td>\n",
       "      <td>-0.009642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.008617</td>\n",
       "      <td>0.008604</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.004666</td>\n",
       "      <td>0.004662</td>\n",
       "      <td>0.004621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.002460</td>\n",
       "      <td>-0.002446</td>\n",
       "      <td>-0.002310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.004280</td>\n",
       "      <td>-0.004270</td>\n",
       "      <td>-0.004175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.000858</td>\n",
       "      <td>-0.000849</td>\n",
       "      <td>-0.000768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.009298</td>\n",
       "      <td>-0.009276</td>\n",
       "      <td>-0.009061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.001059</td>\n",
       "      <td>0.001057</td>\n",
       "      <td>0.001039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.000517</td>\n",
       "      <td>-0.000530</td>\n",
       "      <td>-0.000649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.006874</td>\n",
       "      <td>0.006833</td>\n",
       "      <td>0.006453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.006805</td>\n",
       "      <td>0.006787</td>\n",
       "      <td>0.006608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.002525</td>\n",
       "      <td>-0.002500</td>\n",
       "      <td>-0.002264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.003665</td>\n",
       "      <td>-0.003649</td>\n",
       "      <td>-0.003487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.011076</td>\n",
       "      <td>0.011055</td>\n",
       "      <td>0.010847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.003756</td>\n",
       "      <td>0.003754</td>\n",
       "      <td>0.003730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.002098</td>\n",
       "      <td>0.002080</td>\n",
       "      <td>0.001913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.007004</td>\n",
       "      <td>0.006990</td>\n",
       "      <td>0.006847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.008330</td>\n",
       "      <td>0.008319</td>\n",
       "      <td>0.008215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.002653</td>\n",
       "      <td>-0.002652</td>\n",
       "      <td>-0.002640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7521</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7522</th>\n",
       "      <td>-0.003394</td>\n",
       "      <td>-0.003395</td>\n",
       "      <td>-0.003403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7523</th>\n",
       "      <td>-0.006014</td>\n",
       "      <td>-0.006002</td>\n",
       "      <td>-0.005884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7524</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7525</th>\n",
       "      <td>0.002935</td>\n",
       "      <td>0.002923</td>\n",
       "      <td>0.002802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7526</th>\n",
       "      <td>0.002489</td>\n",
       "      <td>0.002486</td>\n",
       "      <td>0.002454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7527</th>\n",
       "      <td>-0.002794</td>\n",
       "      <td>-0.002781</td>\n",
       "      <td>-0.002657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7528</th>\n",
       "      <td>0.002507</td>\n",
       "      <td>0.002504</td>\n",
       "      <td>0.002480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7529</th>\n",
       "      <td>0.003987</td>\n",
       "      <td>0.003973</td>\n",
       "      <td>0.003845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7530</th>\n",
       "      <td>-0.005353</td>\n",
       "      <td>-0.005331</td>\n",
       "      <td>-0.005124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7531</th>\n",
       "      <td>0.015240</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>0.014824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7532</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7533</th>\n",
       "      <td>0.003551</td>\n",
       "      <td>0.003540</td>\n",
       "      <td>0.003433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7534</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7535</th>\n",
       "      <td>0.005502</td>\n",
       "      <td>0.005499</td>\n",
       "      <td>0.005466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7536</th>\n",
       "      <td>-0.001817</td>\n",
       "      <td>-0.001802</td>\n",
       "      <td>-0.001656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7537</th>\n",
       "      <td>-0.002859</td>\n",
       "      <td>-0.002857</td>\n",
       "      <td>-0.002838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7538</th>\n",
       "      <td>-0.008049</td>\n",
       "      <td>-0.008036</td>\n",
       "      <td>-0.007900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7539</th>\n",
       "      <td>0.004008</td>\n",
       "      <td>0.003992</td>\n",
       "      <td>0.003841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7540</th>\n",
       "      <td>-0.000743</td>\n",
       "      <td>-0.000749</td>\n",
       "      <td>-0.000797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7541</th>\n",
       "      <td>0.010649</td>\n",
       "      <td>0.010630</td>\n",
       "      <td>0.010452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7542</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7543</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7544</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7545</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7546</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7547</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7548</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7549</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7550</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7551 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Small Alpha  Medium Alpha  Large Alpha\n",
       "0       -0.004345     -0.004345    -0.004351\n",
       "1       -0.004591     -0.004589    -0.004572\n",
       "2        0.012339      0.012302     0.011950\n",
       "3        0.005145      0.005133     0.005017\n",
       "4        0.004154      0.004154     0.004155\n",
       "5        0.008317      0.008304     0.008185\n",
       "6        0.001916      0.001919     0.001946\n",
       "7        0.001244      0.001256     0.001367\n",
       "8       -0.012271     -0.012249    -0.012034\n",
       "9       -0.009887     -0.009864    -0.009642\n",
       "10       0.008617      0.008604     0.008475\n",
       "11       0.004666      0.004662     0.004621\n",
       "12      -0.002460     -0.002446    -0.002310\n",
       "13      -0.004280     -0.004270    -0.004175\n",
       "14      -0.000858     -0.000849    -0.000768\n",
       "15      -0.009298     -0.009276    -0.009061\n",
       "16       0.001059      0.001057     0.001039\n",
       "17      -0.000517     -0.000530    -0.000649\n",
       "18       0.006874      0.006833     0.006453\n",
       "19       0.006805      0.006787     0.006608\n",
       "20       0.000000      0.000000     0.000000\n",
       "21      -0.002525     -0.002500    -0.002264\n",
       "22      -0.003665     -0.003649    -0.003487\n",
       "23       0.000000      0.000000     0.000000\n",
       "24       0.011076      0.011055     0.010847\n",
       "25       0.003756      0.003754     0.003730\n",
       "26       0.002098      0.002080     0.001913\n",
       "27       0.007004      0.006990     0.006847\n",
       "28       0.008330      0.008319     0.008215\n",
       "29      -0.002653     -0.002652    -0.002640\n",
       "...           ...           ...          ...\n",
       "7521     0.000000      0.000000     0.000000\n",
       "7522    -0.003394     -0.003395    -0.003403\n",
       "7523    -0.006014     -0.006002    -0.005884\n",
       "7524     0.000000      0.000000     0.000000\n",
       "7525     0.002935      0.002923     0.002802\n",
       "7526     0.002489      0.002486     0.002454\n",
       "7527    -0.002794     -0.002781    -0.002657\n",
       "7528     0.002507      0.002504     0.002480\n",
       "7529     0.003987      0.003973     0.003845\n",
       "7530    -0.005353     -0.005331    -0.005124\n",
       "7531     0.015240      0.015200     0.014824\n",
       "7532     0.000000      0.000000     0.000000\n",
       "7533     0.003551      0.003540     0.003433\n",
       "7534     0.000000      0.000000     0.000000\n",
       "7535     0.005502      0.005499     0.005466\n",
       "7536    -0.001817     -0.001802    -0.001656\n",
       "7537    -0.002859     -0.002857    -0.002838\n",
       "7538    -0.008049     -0.008036    -0.007900\n",
       "7539     0.004008      0.003992     0.003841\n",
       "7540    -0.000743     -0.000749    -0.000797\n",
       "7541     0.010649      0.010630     0.010452\n",
       "7542     0.000000      0.000000     0.000000\n",
       "7543     0.000000      0.000000     0.000000\n",
       "7544     0.000000      0.000000     0.000000\n",
       "7545     0.000000      0.000000     0.000000\n",
       "7546     0.000000      0.000000     0.000000\n",
       "7547     0.000000      0.000000     0.000000\n",
       "7548     0.000000      0.000000     0.000000\n",
       "7549     0.000000      0.000000     0.000000\n",
       "7550     0.000000      0.000000     0.000000\n",
       "\n",
       "[7551 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## PREPROCCESSING ##\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "abDataScale = preprocessing.scale(abData) #Scaling before Ridge so that independent variables are not penalized differently\n",
    "\n",
    "ridgeSmall = Ridge(alpha = 0.1)\n",
    "ridgeMed   = Ridge(alpha = 1.0)\n",
    "ridgeLarge = Ridge(alpha = 10.0)\n",
    "\n",
    "ridgeSmall.fit(abDataScale,abRel)\n",
    "ridgeMed.fit(abDataScale,abRel)\n",
    "ridgeLarge.fit(abDataScale,abRel)\n",
    "\n",
    "## Data Viz ##\n",
    "import pandas as pd\n",
    "\n",
    "small = pd.Series(ridgeSmall.coef_.reshape(7551,))\n",
    "med = pd.Series(ridgeMed.coef_.reshape(7551,))\n",
    "large = pd.Series(ridgeLarge.coef_.reshape(7551,))\n",
    "\n",
    "alphaChart = pd.DataFrame(pd.concat([small,med, large], axis = 1))\n",
    "alphaChart.columns = [\"Small Alpha\",\"Medium Alpha\", \"Large Alpha\"]\n",
    "alphaChart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ufunc 'subtract' did not contain a loop with signature matching types dtype('<U32') dtype('<U32') dtype('<U32')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-7a94bad090f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mridgeLarge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabDataScale\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabRel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m         return r2_score(y, self.predict(X), sample_weight=sample_weight,\n\u001b[1;32m--> 388\u001b[1;33m                         multioutput='variance_weighted')\n\u001b[0m\u001b[0;32m    389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\metrics\\regression.py\u001b[0m in \u001b[0;36mr2_score\u001b[1;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 538\u001b[1;33m     numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0,\n\u001b[0m\u001b[0;32m    539\u001b[0m                                                       dtype=np.float64)\n\u001b[0;32m    540\u001b[0m     denominator = (weight * (y_true - np.average(\n",
      "\u001b[1;31mTypeError\u001b[0m: ufunc 'subtract' did not contain a loop with signature matching types dtype('<U32') dtype('<U32') dtype('<U32')"
     ]
    }
   ],
   "source": [
    "ridgeLarge.score(np.array(abDataScale), np.array(abRel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.68664034, -1.29828671,  0.63542437, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.38643967,  0.09583183, -0.85752847, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.66296341, -0.05939487,  0.8548012 , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ..., \n",
       "       [-0.83910196, -2.06478721, -0.83330029, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.8114011 ,  0.98262876, -0.08454345, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.48874245,  0.2754206 , -1.38495004, ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(abData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEICAYAAABcVE8dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20183c1a198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertical Line at 20 PrComps\n"
     ]
    }
   ],
   "source": [
    "var = 0\n",
    "varExp = []\n",
    "for i in pca.explained_variance_ratio_:\n",
    "    var = var + i\n",
    "    varExp.append(var)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(len(pca.explained_variance_ratio_)), varExp)\n",
    "plt.plot([20,20], [0,1])\n",
    "plt.title(\"AB Data Scree Plot\")\n",
    "plt.show()\n",
    "print(\"Vertical Line at 20 PrComps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca20 = PCA(n_components = 20)\n",
    "pcaData = pca20.fit_transform(abData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288, 20)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcaData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Score\n",
      "[[94  0  1]\n",
      " [ 2 92  0]\n",
      " [ 3  1 86]]\n",
      "Accuracy:  0.974910394265\n",
      "\n",
      "AB Test Score\n",
      "[[0 0 0]\n",
      " [0 2 0]\n",
      " [1 3 2]]\n",
      "Accuracy:  0.5\n",
      "\n",
      "CD Test Score\n",
      "[[35 30 31]\n",
      " [32 29 35]\n",
      " [27 38 31]]\n",
      "Accuracy:  0.329861111111\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators = 10)\n",
    "rf.fit(pcaData[1:280,],abRel[1:280])\n",
    "\n",
    "#Score Training Acc, AB Testing Acc and CD Testing Acc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"\\nTraining Score\")\n",
    "abTest = confusion_matrix(y_true = abRel[1:280],y_pred=rf.predict(pcaData[1:280,]))\n",
    "print(abTest)\n",
    "print(\"Accuracy: \", (abTest[0,0]+abTest[1,1]+abTest[2,2])/len(abRel[1:280]))\n",
    "\n",
    "print(\"\\nAB Test Score\")\n",
    "abTest = confusion_matrix(y_true = abRel[280:],y_pred=rf.predict(pcaData[280:,]))\n",
    "print(abTest)\n",
    "print(\"Accuracy: \", (abTest[0,0]+abTest[1,1]+abTest[2,2])/len(abRel[280:]))\n",
    "\n",
    "cdDataScale = pca20.fit_transform(cdData)\n",
    "print(\"\\nCD Test Score\")\n",
    "abTest = confusion_matrix(y_true = cdRel,y_pred=rf.predict(cdDataScale))\n",
    "print(abTest)\n",
    "print(\"Accuracy: \", (abTest[0,0]+abTest[1,1]+abTest[2,2])/len(cdRel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest - Scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators = 50)\n",
    "rf.fit(abDataScale[1:280,],abRel[1:280])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Score\n",
      "[[95  0  0]\n",
      " [ 0 94  0]\n",
      " [ 0  0 90]]\n",
      "Accuracy:  1.0\n",
      "\n",
      "AB Test Score\n",
      "[[0 0 0]\n",
      " [0 2 0]\n",
      " [1 1 4]]\n",
      "Accuracy:  0.75\n",
      "\n",
      "CD Test Score\n",
      "[[40 30 26]\n",
      " [31 34 31]\n",
      " [32 33 31]]\n",
      "Accuracy:  0.364583333333\n"
     ]
    }
   ],
   "source": [
    "#Score Training Acc, AB Testing Acc and CD Testing Acc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"\\nTraining Score\")\n",
    "abTest = confusion_matrix(y_true = abRel[1:280],y_pred=rf.predict(abDataScale[1:280,]))\n",
    "print(abTest)\n",
    "print(\"Accuracy: \", (abTest[0,0]+abTest[1,1]+abTest[2,2])/len(abRel[1:280]))\n",
    "\n",
    "print(\"\\nAB Test Score\")\n",
    "abTest = confusion_matrix(y_true = abRel[280:],y_pred=rf.predict(abDataScale[280:,]))\n",
    "print(abTest)\n",
    "print(\"Accuracy: \", (abTest[0,0]+abTest[1,1]+abTest[2,2])/len(abRel[280:]))\n",
    "\n",
    "cdDataScale = preprocessing.scale(cdData)\n",
    "print(\"\\nCD Test Score\")\n",
    "abTest = confusion_matrix(y_true = cdRel,y_pred=rf.predict(cdDataScale))\n",
    "print(abTest)\n",
    "print(\"Accuracy: \", (abTest[0,0]+abTest[1,1]+abTest[2,2])/len(cdRel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest - Unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Score\n",
      "[[95  0  0]\n",
      " [ 0 94  0]\n",
      " [ 0  0 90]]\n",
      "Accuracy:  1.0\n",
      "\n",
      "AB Test Score\n",
      "[[0 0 0]\n",
      " [0 2 0]\n",
      " [1 1 4]]\n",
      "Accuracy:  0.75\n",
      "\n",
      "CD Test Score\n",
      "[[39 28 29]\n",
      " [30 37 29]\n",
      " [28 36 32]]\n",
      "Accuracy:  0.375\n"
     ]
    }
   ],
   "source": [
    "## Unscaled Data ##\n",
    "rf = RandomForestClassifier(n_estimators = 500)\n",
    "rf.fit(abData[1:280,],abRel[1:280])\n",
    "\n",
    "print(\"\\nTraining Score\")\n",
    "abTest = confusion_matrix(y_true = abRel[1:280],y_pred=rf.predict(abData[1:280,]))\n",
    "print(abTest)\n",
    "print(\"Accuracy: \", (abTest[0,0]+abTest[1,1]+abTest[2,2])/len(abRel[1:280]))\n",
    "\n",
    "print(\"\\nAB Test Score\")\n",
    "abTest = confusion_matrix(y_true = abRel[280:],y_pred=rf.predict(abData[280:,]))\n",
    "print(abTest)\n",
    "print(\"Accuracy: \", (abTest[0,0]+abTest[1,1]+abTest[2,2])/len(abRel[280:]))\n",
    "\n",
    "print(\"\\nCD Test Score\")\n",
    "abTest = confusion_matrix(y_true = cdRel,y_pred=rf.predict(cdData))\n",
    "print(abTest)\n",
    "print(\"Accuracy: \", (abTest[0,0]+abTest[1,1]+abTest[2,2])/len(cdRel))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Chi^2 Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "abDataScalePos = abDataScale - np.amin(abDataScale) #Minus bc its a negative number\n",
    "\n",
    "stat, pval = chi2(abDataScalePos,abRel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest P Val: 0.295052070186\n"
     ]
    }
   ],
   "source": [
    "chiChart = pd.DataFrame(data = pd.concat([pd.Series(stat),pd.Series(pval)], axis = 1))\n",
    "chiChart.columns = [\"Chi2 Val\",\"P Val\"]\n",
    "print(\"Smallest P Val:\", np.amin(pval))\n",
    "chiChart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adam1brownell\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "              solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as lda\n",
    "\n",
    "lda_svd = lda()\n",
    "lda_svd.fit(abDataScale,abRel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stepwise Regression due to Collinearity: Least Angle Regression (LARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lars(copy_X=True, eps=2.2204460492503131e-16, fit_intercept=True,\n",
       "   fit_path=True, n_nonzero_coefs=500, normalize=True, positive=False,\n",
       "   precompute='auto', verbose=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## LARS is similar to forward stepwise regression\n",
    "from sklearn.linear_model import Lars\n",
    "\n",
    "stepwise = Lars()\n",
    "stepwise.fit(abData,abRel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Reduced Dimensionality from 7551 to 388\n",
    "abDataSelect = abDataScale[:,stepwise.active_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Score\n",
      "[[95  0  0]\n",
      " [ 0 94  0]\n",
      " [ 0  0 90]]\n",
      "Accuracy:  1.0\n",
      "\n",
      "AB Test Score\n",
      "[[0 0 0]\n",
      " [0 0 2]\n",
      " [2 0 4]]\n",
      "Accuracy:  0.5\n",
      "\n",
      "CD Test Score\n",
      "[[41 29 26]\n",
      " [32 36 28]\n",
      " [29 35 32]]\n",
      "Accuracy:  0.378472222222\n"
     ]
    }
   ],
   "source": [
    "## LARS data rf ##\n",
    "rf = RandomForestClassifier(n_estimators = 50)\n",
    "rf.fit(abDataSelect[1:280,],abRel[1:280])\n",
    "\n",
    "print(\"\\nTraining Score\")\n",
    "abTest = confusion_matrix(y_true = abRel[1:280],y_pred=rf.predict(abDataSelect[1:280,]))\n",
    "print(abTest)\n",
    "print(\"Accuracy: \", (abTest[0,0]+abTest[1,1]+abTest[2,2])/len(abRel[1:280]))\n",
    "\n",
    "print(\"\\nAB Test Score\")\n",
    "abTest = confusion_matrix(y_true = abRel[280:],y_pred=rf.predict(abDataSelect[280:,]))\n",
    "print(abTest)\n",
    "print(\"Accuracy: \", (abTest[0,0]+abTest[1,1]+abTest[2,2])/len(abRel[280:]))\n",
    "\n",
    "cdDataSelect = cdDataScale[:,stepwise.active_]\n",
    "print(\"\\nCD Test Score\")\n",
    "abTest = confusion_matrix(y_true = cdRel,y_pred=rf.predict(cdDataSelect))\n",
    "print(abTest)\n",
    "print(\"Accuracy: \", (abTest[0,0]+abTest[1,1]+abTest[2,2])/len(cdRel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stepwise Regression on CD Data to compare to AB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lars(copy_X=True, eps=2.2204460492503131e-16, fit_intercept=True,\n",
       "   fit_path=True, n_nonzero_coefs=500, normalize=True, positive=False,\n",
       "   precompute='auto', verbose=False)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CDstepwise = Lars()\n",
    "CDstepwise.fit(cdData,cdRel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CDstepwise.active_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stepwise2 = []\n",
    "for i in stepwise.active_:\n",
    "    if i in CDstepwise.active_:\n",
    "        stepwise2.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stepwise2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5838,\n",
       " 1382,\n",
       " 6035,\n",
       " 301,\n",
       " 476,\n",
       " 5187,\n",
       " 1995,\n",
       " 7420,\n",
       " 629,\n",
       " 3574,\n",
       " 3207,\n",
       " 2263,\n",
       " 4173,\n",
       " 6214,\n",
       " 6606,\n",
       " 4769,\n",
       " 402,\n",
       " 3511,\n",
       " 6985,\n",
       " 4682,\n",
       " 5568,\n",
       " 5461,\n",
       " 5338,\n",
       " 659,\n",
       " 620,\n",
       " 3829,\n",
       " 2778,\n",
       " 2155,\n",
       " 3130,\n",
       " 1604,\n",
       " 1063,\n",
       " 6473,\n",
       " 1753,\n",
       " 4987,\n",
       " 6386,\n",
       " 6021,\n",
       " 6569,\n",
       " 3347]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stepwise2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare P vals from Conjoined Stepwise and AB Stepwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AB Stepwise, Test Score\n",
      "[[41 30 25]\n",
      " [32 37 27]\n",
      " [31 34 31]]\n",
      "Accuracy:  0.378472222222\n",
      "\n",
      "CD Stepwise, Test Score\n",
      "[[42 32 22]\n",
      " [29 41 26]\n",
      " [32 37 27]]\n",
      "Accuracy:  0.381944444444\n",
      "\n",
      "Combined Stepwise, Test Score\n",
      "[[30 31 35]\n",
      " [28 38 30]\n",
      " [36 36 24]]\n",
      "Accuracy:  0.319444444444\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 50)\n",
    "rf.fit(abDataScale[:,stepwise.active_],abRel)\n",
    "\n",
    "cdDataSelect = cdDataScale[:,stepwise.active_]\n",
    "print(\"\\nAB Stepwise, Test Score\")\n",
    "abTest = confusion_matrix(y_true = cdRel,y_pred=rf.predict(cdDataSelect))\n",
    "print(abTest)\n",
    "print(\"Accuracy: \", (abTest[0,0]+abTest[1,1]+abTest[2,2])/len(cdRel))\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = 50)\n",
    "rf.fit(abDataScale[:,CDstepwise.active_],abRel)\n",
    "\n",
    "\n",
    "cdDataSelect = cdDataScale[:,CDstepwise.active_]\n",
    "print(\"\\nCD Stepwise, Test Score\")\n",
    "abTest = confusion_matrix(y_true = cdRel,y_pred=rf.predict(cdDataSelect))\n",
    "print(abTest)\n",
    "print(\"Accuracy: \", (abTest[0,0]+abTest[1,1]+abTest[2,2])/len(cdRel))\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = 50)\n",
    "rf.fit(abDataScale[:,stepwise2],abRel)\n",
    "\n",
    "\n",
    "cdDataSelect = cdDataScale[:,stepwise2]\n",
    "print(\"\\nCombined Stepwise, Test Score\")\n",
    "abTest = confusion_matrix(y_true = cdRel,y_pred=rf.predict(cdDataSelect))\n",
    "print(abTest)\n",
    "print(\"Accuracy: \", (abTest[0,0]+abTest[1,1]+abTest[2,2])/len(cdRel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fighting The Free Lunch: Trying above with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AB Stepwise, Test Score\n",
      "[[30 25 41]\n",
      " [32 24 40]\n",
      " [38 30 28]]\n",
      "Accuracy:  0.284722222222\n",
      "\n",
      "CD Stepwise, Test Score\n",
      "[[26 33 37]\n",
      " [26 25 45]\n",
      " [34 41 21]]\n",
      "Accuracy:  0.25\n",
      "\n",
      "Combined Stepwise, Test Score\n",
      "[[37 26 33]\n",
      " [29 33 34]\n",
      " [39 19 38]]\n",
      "Accuracy:  0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adam1brownell\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "lda_svd = lda()\n",
    "lda_svd.fit(abDataScale[:,stepwise.active_],abRel)\n",
    "\n",
    "cdDataSelect = cdDataScale[:,stepwise.active_]\n",
    "print(\"\\nAB Stepwise, Test Score\")\n",
    "abTest = confusion_matrix(y_true = cdRel,y_pred=lda_svd.predict(cdDataSelect))\n",
    "print(abTest)\n",
    "print(\"Accuracy: \", (abTest[0,0]+abTest[1,1]+abTest[2,2])/len(cdRel))\n",
    "\n",
    "lda_svd = lda()\n",
    "lda_svd.fit(abDataScale[:,CDstepwise.active_],abRel)\n",
    "\n",
    "cdDataSelect = cdDataScale[:,CDstepwise.active_]\n",
    "print(\"\\nCD Stepwise, Test Score\")\n",
    "abTest = confusion_matrix(y_true = cdRel,y_pred=lda_svd.predict(cdDataSelect))\n",
    "print(abTest)\n",
    "print(\"Accuracy: \", (abTest[0,0]+abTest[1,1]+abTest[2,2])/len(cdRel))\n",
    "\n",
    "\n",
    "lda_svd = lda()\n",
    "lda_svd.fit(abDataScale[:,stepwise2],abRel)\n",
    "\n",
    "cdDataSelect = cdDataScale[:,stepwise2]\n",
    "print(\"\\nCombined Stepwise, Test Score\")\n",
    "abTest = confusion_matrix(y_true = cdRel,y_pred=lda_svd.predict(cdDataSelect))\n",
    "print(abTest)\n",
    "print(\"Accuracy: \", (abTest[0,0]+abTest[1,1]+abTest[2,2])/len(cdRel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
